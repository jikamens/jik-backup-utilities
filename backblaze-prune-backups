#!/usr/bin/env python3

import argparse
from b2api import B2API
import datetime
import fnmatch
import json
import logging
import os
import psutil
from requests.exceptions import HTTPError
from rcloneutils import (
    get_encrypted_remote,
    get_plain_remote,
    get_rclone_config,
    RcloneDecoder,
)
import re
import resource
import sys
import threading
import time

whoami = os.path.basename(sys.argv[0])
one_day = 60 * 60 * 24
max_fds = resource.getrlimit(resource.RLIMIT_NOFILE)[0]
my_process = psutil.Process()

logger = None
worker_semaphore = threading.Semaphore(0)
worker_queue = []
worker_queue_semaphore = threading.Semaphore(0)
worker_queue_lock = threading.RLock()
failed_thread_count = 0
daemon_threads = set()
last_thread_launch = 0
min_threads = 10

policies = {
    # Each number here means a number of days, so for example, in the "default"
    # entry above, we save one deleted version for each day of the first week.
    # '*' means to repeat the previous number until doing so would exceed the
    # next number, so for example "default" expands to 1, 2, 3, 4, 5, 6, 7, 14,
    # 21, 28, 30, 60, etc.
    # '?' means the same as '*' but only if the file is currently undeleted, so
    # for example in "default" we only save previous versions of deleted files
    # for 365 days and then purge them.
    # If the rule doesn't end with '*' or '?', then everything older than the
    # last specified age is removed.
    'default': (1, 2, 3, 4, 5, 6, 7, '*', 30, '*', 365, '?'),
    'never': (),
    'one_week': (1, 2, 3, 4, 5, 6, 7),
    'one_month': (1, 2, 3, 4, 5, 6, 7, '*', 30),
    'three_months': (1, 2, 3, 4, 5, 6, 7, '*', 30, '*', 91),
    'six_months': (1, 2, 3, 4, 5, 6, 7, '*', 30, '*', 182),
    'one_year': (1, 2, 3, 4, 5, 6, 7, '*', 30, '*', 365),
    'one_year_all': (1, '*', 365),
}

# You can use glob patterns here or constant paths. The pattern or path needs
# to match (according to `fnmatch.fnmatch`) the start of the file path it's
# being compared to.
path_policies = {
    '*/.git/': 'one_week',
    'archives/': 'one_year',
    'isos/': 'one_year',
    'mac-backup/*/Applications/': 'one_week',
    'pictures/': 'one_year',
}

purge_policies = {
    '': 'never',
}

constant_path_policies = None
fnmatch_path_policies = None


class DeleteFileException(Exception):
    pass


class PurgePolicy(object):
    now = int(time.time())

    def __init__(self, spec, oldest):
        self.original_spec = spec
        self.spec = self.expand_spec(spec, oldest)

    def expand_spec(self, original_spec, oldest):
        # Each expanded spec element is [reason, days, cutoff, must_exist]
        spec = []
        for i in range(len(original_spec)):
            orig = original_spec[i]
            if orig in ('*', '?'):
                multiplier = 2
                before = spec[-1][1]
                try:
                    after = original_spec[i + 1]
                except IndexError:
                    after = int((self.now - oldest) / one_day) + 1 + before
                while before * multiplier <= after:
                    spec.append(['{}x{}{}'.format(before, multiplier, orig),
                                 before * multiplier,
                                 self.now - one_day * before * multiplier,
                                 False if orig == '*' else True])
                    multiplier += 1
            else:
                spec.append(
                    [str(orig), orig, self.now - one_day * orig, False])
        logger.debug(f'Expanded spec {original_spec} to {spec}')
        return spec

    def apply(self, files):
        # Versions are guaranteed to be in reverse chronological order (newest
        # first) when they arrive here from b2_list_file_versions.
        if files[0]['b2_action'] == 'upload':
            is_undeleted = True
            files.pop(0)
        else:
            is_undeleted = False
        for file in files:
            if file['b2_action'] == 'hide':
                file['action'] = 'preserve'
                file['reason'] = 'hide still needed'
            else:
                file['action'] = 'delete'
                file['reason'] = ''
        # Save the newest and oldest version within each save window.
        prev_cutoff = self.now
        for cutoff in self.spec:
            in_window = list(f for f in files
                             if f['b2_action'] == 'upload' and
                             prev_cutoff >= f['timestamp'] >= cutoff[2])
            prev_cutoff = cutoff[2]
            if not in_window:
                continue
            if cutoff[3] and not is_undeleted:
                for f in in_window:
                    f['action'] = 'delete'
                    f['reason'] = cutoff[0] + ' and file is gone'
                continue
            in_window[-1]['action'] = 'preserve'
            in_window[-1]['reason'] = cutoff[0] + ' oldest'
            in_window[0]['action'] = 'preserve'
            in_window[0]['reason'] = cutoff[0] + ' newest'
        # Delete any hides that no longer need to be preserved.
        have_upload = False
        for f in reversed(files):
            if f['b2_action'] == 'upload':
                if f['action'] == 'preserve':
                    have_upload = True
            else:
                if have_upload:
                    have_upload = False
                else:
                    f['action'] = 'delete'
                    f['reason'] = 'hide no longer needed'


def find_path_policy(path):
    for policy_path, policy in constant_path_policies.items():
        if path.startswith(policy_path):
            return policy
    for policy_path, policy in fnmatch_path_policies.items():
        if fnmatch.fnmatch(path, policy_path):
            return policy
    return 'default'


def delete_one_file(args, account_id, api_key, decoded_path, path, timestamp,
                    file_id):
    global last_thread_launch
    if not clean_threads(args):
        raise DeleteFileException('clean_threads failed')
    if not worker_semaphore.acquire(False):
        # Don't exceed 50% of available file descriptors
        open_fds = my_process.num_fds()
        if ((args.max_threads and len(daemon_threads) >= args.max_threads) or
            open_fds / max_fds >= .5 or
            # Launch no more than one new thread per second. This prevents a
            # burst of file-deletion requests causing too many threads to be
            # created, causing us to be blocked on the server.
            (len(daemon_threads) >= min_threads and
             time.time() - last_thread_launch < 1)):
            logger.debug('Waiting for worker to become available')
        else:
            logger.debug(f'Starting new thread ({len(daemon_threads)} active)')
            t = threading.Thread(target=delete_worker_wrapper,
                                 args=(args, account_id, api_key),
                                 daemon=True)
            daemon_threads.add(t)
            t.start()
            last_thread_launch = time.time()
    else:
        worker_semaphore.release()
        logger.debug('Using existing thread')
    worker_queue_lock.acquire()
    worker_queue.append((decoded_path, path, timestamp, file_id))
    worker_queue_lock.release()
    worker_queue_semaphore.release()


def delete_worker_wrapper(args, account_id, api_key):
    try:
        delete_worker(args, account_id, api_key)
    except Exception:
        logger.exception('Exception from delete worker')


def delete_worker(args, account_id, api_key):
    b2 = B2API(account_id, api_key, logger=logger)
    worker_semaphore.release()
    logger.debug('Thread active')
    while worker_queue_semaphore.acquire():
        logger.debug('Thread working')
        worker_semaphore.acquire()
        worker_queue_lock.acquire()
        decoded_path, path, timestamp, file_id = worker_queue.pop(0)
        worker_queue_lock.release()
        if decoded_path is None:
            logger.debug('Thread exiting')
            return
        try:
            if args.dryrun:
                b2.get_file_info(file_id)
                action = 'Would delete'
            else:
                b2.delete_file_version(path, file_id)
                action = 'Deleted'
            logger.debug(f'{action} {decoded_path} timestamp {timestamp} '
                         f'file ID {file_id}')
        except HTTPError as e:
            response = e.response
            try:
                obj = json.loads(response.content)
            except Exception:
                raise e
            if obj.get('code', None) == 'file_not_present':
                logger.error(
                    f'file_not_present error deleting version {file_id} of '
                    f'{decoded_path} (encrypted: {path}); probably removed '
                    f'in an earlier call which timed out')
            else:
                raise
        except Exception:
            logger.exception(f'Exception deleting version {file_id} of '
                             f'{decoded_path} (encrypted: {path})')
            return
        finally:
            worker_semaphore.release()
        logger.debug('Thread waiting')


def do_one_file(args, path, versions, decoder, account_id, api_key):
    if not path:
        return True
    decoded_path = decoder.get(path)
    if decoded_path == args.stop_at:
        logger.info(f'Stopping at {decoded_path}')
        return False
    if not versions or \
       (len(versions) == 1 and versions[0]['b2_action'] == 'upload'):
        logger.debug(f'File {decoded_path} does not have any deleted versions')
        return True
    policy_name = find_path_policy(decoded_path)
    policy = PurgePolicy(policies[policy_name],
                         min(v['timestamp'] for v in versions))
    logger.debug(f'Applying policy {policy_name} to {decoded_path}')
    policy.apply(versions)
    if args.verbose:
        print('Applied policy {} to {}:'.format(policy_name, decoded_path))
    for version in reversed(versions):
        timestamp = str(datetime.datetime.fromtimestamp(version['timestamp']))
        action = version['action']
        msg = version['action']
        if version['reason']:
            msg += ' (' + version['reason'] + ')'
        logger.debug(f'{decoded_path} version {timestamp} result: {msg}')
        if args.verbose:
            print('  {} {}'.format(timestamp, msg))
        if action == 'delete':
            delete_one_file(args, account_id, api_key, decoded_path,
                            path, timestamp, version['file_id'])
    return True


def parse_args():
    global constant_path_policies, fnmatch_path_policies
    parser = argparse.ArgumentParser(description='Rule-based removal of old '
                                     'rclone-encrypted B2 backup files')
    parser.add_argument('--debug', action='store_true', default=False,
                        help='Enable debug logging')
    parser.add_argument('--verbose', action='store_true', default=False,
                        help='Enable verbose output')
    parser.add_argument('--dryrun', action='store_true', default=False,
                        help='Say what would be done without doing it')
    parser.add_argument('--purge', action='store_true', default=False,
                        help='Purge all deleted files (requires --force!)')
    parser.add_argument('--force', action='store_true', default=False,
                        help='Force dangerous operations like --purge')
    parser.add_argument('--start-at', action='store',
                        help='Path to start with')
    parser.add_argument('--stop-at', action='store',
                        help='Path to stop with')
    parser.add_argument('--max-files', action='store', type=int, help='Stop '
                        'after processing at most this many files')
    parser.add_argument('--max-failed-threads', type=int, action='store',
                        default=100, help='Maximum number of thread failures '
                        'to allow before giving up (default 100)')
    parser.add_argument('--max-threads', type=int, action='store',
                        help='Maximum number of worker threads')
    parser.add_argument('--no-files', action='store_false', default=True,
                        dest='do_files', help="Don't clean up old files, "
                        "just do unfinished large files")
    parser.add_argument('--no-unfinished-large-files', action='store_false',
                        default=True, dest='do_unfinished', help="Don't "
                        "clean up unfinished large files, just do old files")
    parser.add_argument('remote', nargs='?', help='Name of rclone crypt '
                        'remote backed by B2 to prune (if not specified, '
                        'found dynamically in ~/.rclone.conf)')
    args = parser.parse_args()
    if not (args.do_files or args.do_unfinished):
        parser.error("Don't specify both --no-files and "
                     "--no-unfinished-large-files")

    if args.purge:
        if not args.force:
            parser.error('--purge requires --force')
        policies = purge_policies
    else:
        policies = path_policies

    constant_path_policies = {pp: p for pp, p in policies.items()
                              if not re.search(r'[\[*?]', pp)}
    fnmatch_path_policies = {pp: p for pp, p in policies.items()
                             if pp not in constant_path_policies}

    return args


def main():
    global logger

    args = parse_args()
    logger = logging.getLogger(whoami)
    logger.setLevel(logging.DEBUG if args.debug else logging.INFO)
    formatter = logging.Formatter(
        fmt='%(asctime)s %(levelname)s: %(name)s[%(process)d.%(thread)d]: '
        '%(message)s')
    handler = logging.StreamHandler(stream=sys.stderr)
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    if args.verbose:
        print('Starting at {}'.format(datetime.datetime.now()))

    config = get_rclone_config()
    if not args.remote:
        args.remote = get_encrypted_remote(config)
        if args.verbose:
            print('Using remote {}'.format(args.remote))
    b2, bucket_id = get_plain_remote(config, args.remote, logger=logger)

    logger.debug(f'Using bucket ID {bucket_id}')

    exit_status = 0
    if args.do_files:
        try:
            main_loop(args, b2, bucket_id)
        except DeleteFileException as e:
            exit_status = str(e)

        if not clean_threads(args, final=True):
            exit_status = exit_status or 'clean_threads failed'

    if args.do_unfinished:
        if not do_unfinished_large_files(args, b2, bucket_id):
            exit_status = exit_status or 'do_unfinished_large_files failed'

    if args.verbose:
        print('Finished at {}'.format(datetime.datetime.now()))

    sys.exit(exit_status)


def clean_threads(args, final=False):
    global failed_thread_count

    ok = clean_threads(args) if final else True
    if final:
        for t in daemon_threads:
            logger.debug('Telling thread {} to exit'.format(t.ident))
            worker_queue_lock.acquire()
            worker_queue.append((None, None, None, None))
            worker_queue_lock.release()
            worker_queue_semaphore.release()
    for t in list(daemon_threads):
        if not (t.is_alive() or final):
            logger.error(f'Thread {t.ident} ({t.name}) died prematurely')
            failed_thread_count += 1
            if failed_thread_count == args.max_failed_threads:
                logger.error('Too many failed threads')
                ok = False
        if not t.is_alive() or final:
            t.join()
            daemon_threads.remove(t)
    if final and failed_thread_count > 0:
        logger.error(f'A total of {failed_thread_count} threads failed')
        ok = False
    return ok


def do_unfinished_large_files(args, b2, bucket_id):
    now = datetime.datetime.now(tz=datetime.UTC)
    a_day_ago = now - datetime.timedelta(days=1)
    for f in b2.list_unfinished_large_files(bucket_id):
        when = datetime.datetime.fromtimestamp(f['uploadTimestamp'] / 1000,
                                               tz=datetime.UTC)
        if when > a_day_ago:
            if args.verbose:
                print('Skipping unfinished large file {}, last modified {}'.
                      format(f['fileId'], when))
        else:
            if args.verbose:
                print('Deleting unfinished large file {}, last modified {}'.
                      format(f['fileId'], when))
            if not args.dryrun:
                b2.cancel_large_file(f['fileId'])
    return True


def main_loop(args, b2, bucket_id):
    go_on = True
    last_file_name = None
    queue = []
    last_timestamp = None
    versions = []

    account_id = b2.account_id
    api_key = b2.account_key
    decoder_args = {}
    if args.stop_at or args.max_files:
        # So we notice faster when it's time to stop.
        decoder_args['queue_size'] = 100
    decoder = RcloneDecoder(args.remote, **decoder_args)

    if args.start_at:
        encoder = RcloneDecoder(args.remote, mode='encode')
        encoder.add(args.start_at)
        startFileName = encoder.get(args.start_at)
    else:
        startFileName = None

    processed_count = 0
    for file in b2.list_file_versions(bucket_id, startFileName=startFileName):
        file_name = file['fileName']
        # We want to delete old file versions # days after they were deleted,
        # not # days after they were uploaded, so we use the timestamp of the
        # file version following each version to decide when to delete it.
        #
        # Note that the B2 API endpoint guarantees to return file versions in
        # reverse chronological order.
        timestamp = last_timestamp
        last_timestamp = int(file['uploadTimestamp'] / 1000)
        if timestamp is None:
            timestamp = last_timestamp
        executed = decoder.add(file_name)
        if file_name != last_file_name:
            queue.append([last_file_name, versions])
            if executed or decoder.full:
                for q in queue:
                    go_on = do_one_file(args, q[0], q[1], decoder,
                                        account_id, api_key)
                    processed_count += 1
                    if args.max_files and processed_count >= args.max_files:
                        go_on = False
                    if not go_on:
                        break
                queue = []
            if not go_on:
                break
            versions = []
            last_file_name = file_name
            last_timestamp = None
        if file['action'] not in ('upload', 'hide'):
            continue
        versions.append({
            'file_id': file['fileId'],
            'timestamp': timestamp,
            'b2_action': file['action']})

    if go_on and versions:
        queue.append([last_file_name, versions])
        for q in queue:
            if not do_one_file(args, q[0], q[1], decoder, account_id,
                               api_key):
                break


if __name__ == '__main__':
    main()
